{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming up with Labelling functions\n",
    "\n",
    "This document provides a short overview of .\n",
    "In general I was thinking of features as described in past papers on the topic and was trying to look whether these features can translate into labelling functions.\n",
    "\n",
    "## Using Word2Vec Google pre-trained model\n",
    "In the first part I am looking at pre-trained word embedding model. This model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "trained_model = gensim.models.KeyedVectors.load_word2vec_format('~/models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word + 'negative' - 'positive'\n",
    "\n",
    "My hypothesis was that if we take take a positive word (e.g. enthusiastic) and using word vectors perform the operation, we might get something like this: \n",
    "'enthusiastic' - 'positive' + 'negative' = antonym to enthusiastic  \n",
    "This sometimes actually works, but is obviously not as straightforward. Sometimes we still get synonyms higher on the list than antonyms.  \n",
    "However, I did notice the following property comparing the lists of most similar to *word* (1) and most similar to *word - 'positive' + 'negative* (2). The antonyms tend to have higher rank on (2) than they did on (1) even if they don't reach the top rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51042433387974262"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('proud', 'ashamed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'unenthusiastic', 0.5785017013549805),\n",
       " (u'enthused', 0.5230987071990967),\n",
       " (u'effusive', 0.5033376812934875),\n",
       " (u'enthusiatic', 0.49656665325164795),\n",
       " (u'Enthusiastic', 0.49166086316108704),\n",
       " (u'vociferous', 0.48619210720062256),\n",
       " (u'enthusiastically', 0.4839787483215332),\n",
       " (u'passionate', 0.4743078649044037),\n",
       " (u'apprehensive', 0.47088587284088135),\n",
       " (u'ardent', 0.46236997842788696)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'enthusiastic' - 'positive' + 'negative'\n",
    "trained_model.most_similar(positive=['enthusiastic', 'negative'], negative=['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'enthused', 0.7288056015968323),\n",
       " (u'appreciative', 0.6407038569450378),\n",
       " (u'passionate', 0.6311060786247253),\n",
       " (u'Enthusiastic', 0.6302011013031006),\n",
       " (u'enthusiatic', 0.6265905499458313),\n",
       " (u'enthusiastically', 0.6244474053382874),\n",
       " (u'excited', 0.6102643013000488),\n",
       " (u'unenthusiastic', 0.6077113151550293),\n",
       " (u'ecstatic', 0.6036043167114258),\n",
       " (u'effusive', 0.5879316329956055)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'enthusiastic'\n",
    "trained_model.most_similar(positive=['enthusiastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Balanced', 0.5382355451583862),\n",
       " (u'unbalanced', 0.5257672071456909),\n",
       " (u'imbalanced', 0.45854058861732483),\n",
       " (u'potent_reuptake_inhibitor', 0.45243775844573975),\n",
       " (u'balance', 0.42325860261917114),\n",
       " (u'balancing', 0.4219854176044464),\n",
       " (u'freshly_saut\\xe9ed', 0.41497623920440674),\n",
       " (u'unbalance', 0.4121413230895996),\n",
       " (u'Bayswater_combines', 0.40477824211120605),\n",
       " (u'slanted', 0.3982120156288147)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'balanced' - 'positive' + 'negative'\n",
    "trained_model.most_similar(positive=['balanced', 'negative'], negative=['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Balanced', 0.5727792978286743),\n",
       " (u'potent_reuptake_inhibitor', 0.5593193769454956),\n",
       " (u'consistent', 0.5022857189178467),\n",
       " (u'balance', 0.5000194907188416),\n",
       " (u'ChartPoppers.com_strives', 0.499819815158844),\n",
       " (u'unbalanced', 0.4863540232181549),\n",
       " (u'freshly_saut\\xe9ed', 0.48340001702308655),\n",
       " (u'fiscally_responsible', 0.47322702407836914),\n",
       " (u'balancing', 0.466336190700531),\n",
       " (u'Bayswater_combines', 0.46510744094848633)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'balanced'\n",
    "trained_model.most_similar(positive=['balanced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'unhappy', 0.5862479209899902),\n",
       " (u'glad', 0.5782526731491089),\n",
       " (u'disappointed', 0.5319445729255676),\n",
       " (u'worried', 0.5297830104827881),\n",
       " (u'Said_Hirschbeck', 0.5221741199493408),\n",
       " (u'sorry', 0.5157001614570618),\n",
       " (u'overjoyed', 0.5088183879852295),\n",
       " (u'ecstatic', 0.5060049891471863),\n",
       " (u'numb_Gwen_Bacquet', 0.499439001083374),\n",
       " (u'annoyed', 0.4985652565956116)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'happy' - 'positive' + 'negative'\n",
    "trained_model.most_similar(positive=['happy', 'negative'], negative=['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'glad', 0.7408890128135681),\n",
       " (u'pleased', 0.6632171273231506),\n",
       " (u'ecstatic', 0.6626912355422974),\n",
       " (u'overjoyed', 0.6599286794662476),\n",
       " (u'thrilled', 0.6514049768447876),\n",
       " (u'satisfied', 0.6437950134277344),\n",
       " (u'proud', 0.636042058467865),\n",
       " (u'delighted', 0.627237856388092),\n",
       " (u'disappointed', 0.6269949674606323),\n",
       " (u'excited', 0.6247666478157043)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'happy'\n",
    "trained_model.most_similar(positive=['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'immensely_proud', 0.5935452580451965),\n",
       " (u'thrilled', 0.5332244634628296),\n",
       " (u'ashamed', 0.5059055089950562),\n",
       " (u'Lynn_Coeby', 0.49850228428840637),\n",
       " (u'grateful', 0.4978291392326355),\n",
       " (u'glad', 0.49502453207969666),\n",
       " (u'prouder', 0.4870137870311737),\n",
       " (u'justifiably_proud', 0.4836322069168091),\n",
       " (u'honored', 0.4793172776699066),\n",
       " (u'Ripton_alongside', 0.4778229892253876)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'proud' - 'positive' + 'negative'\n",
    "trained_model.most_similar(positive=['proud', 'negative'], negative=['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'immensely_proud', 0.7941136360168457),\n",
       " (u'thrilled', 0.7283560037612915),\n",
       " (u'pleased', 0.7123605608940125),\n",
       " (u'delighted', 0.7018399238586426),\n",
       " (u'grateful', 0.7007218599319458),\n",
       " (u'excited', 0.6833415627479553),\n",
       " (u'justifiably_proud', 0.6696771383285522),\n",
       " (u'glad', 0.657325029373169),\n",
       " (u'honored', 0.6445347666740417),\n",
       " (u'thankful', 0.6442625522613525)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'proud'\n",
    "trained_model.most_similar(positive=['proud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word + 'positive' - 'negative' \n",
    "This is analogous to the above, but when a negative word is considered as a base and we are looking for a negative antonym. The observations from above still hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'saddening', 0.5947166085243225),\n",
       " (u'bittersweet', 0.5927517414093018),\n",
       " (u'happy', 0.584730863571167),\n",
       " (u'saddened', 0.5550497770309448),\n",
       " (u'heartbreaking', 0.5526055693626404),\n",
       " (u'wonderful', 0.5431721210479736),\n",
       " (u'heartening', 0.5382798910140991),\n",
       " (u'disheartening', 0.5335900187492371),\n",
       " (u'Sad', 0.5243781805038452),\n",
       " (u'tragic', 0.5188040137290955)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'sad' - 'negative' + 'positive'\n",
    "trained_model.most_similar(positive=['sad', 'positive'], negative=['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'saddening', 0.7273085713386536),\n",
       " (u'Sad', 0.6610826253890991),\n",
       " (u'saddened', 0.6604382991790771),\n",
       " (u'heartbreaking', 0.6573507785797119),\n",
       " (u'disheartening', 0.6507317423820496),\n",
       " (u'Meny_Friedman', 0.6487058401107788),\n",
       " (u'parishioner_Pat_Patello', 0.6475859880447388),\n",
       " (u'saddens_me', 0.6407118439674377),\n",
       " (u'distressing', 0.6399092674255371),\n",
       " (u'reminders_bobbing', 0.6357713937759399)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'sad'\n",
    "trained_model.most_similar(positive=['sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'proud', 0.5834987163543701),\n",
       " (u'sorry', 0.5710163116455078),\n",
       " (u'embarrassed', 0.5382394790649414),\n",
       " (u'grateful', 0.5158604383468628),\n",
       " (u'glad', 0.5080356597900391),\n",
       " (u'happy', 0.5078516006469727),\n",
       " (u'thankful', 0.49530261754989624),\n",
       " (u'embarassed', 0.4939783215522766),\n",
       " (u'disgusted', 0.49206268787384033),\n",
       " (u'remorseful', 0.4911426305770874)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'ashamed' - 'negative' + 'positive'\n",
    "trained_model.most_similar(positive=['ashamed', 'positive'], negative=['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, I tried this for a not strongly polar word ('yellow'). As expected this doesn't show anything interesting, just re-shiffles a list of colours. This shows that we need to be careful, in genral that method will not be good for determining wether a word is polar or non-polar, but knowing a polar word it can indicate whether it's positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bright_yellow', 0.6055178642272949),\n",
       " (u'red', 0.5892467498779297),\n",
       " (u'orange', 0.5437914133071899),\n",
       " (u'bright_orange', 0.5282124876976013),\n",
       " (u'pink', 0.5237631797790527),\n",
       " (u'blue', 0.5166007280349731),\n",
       " (u'yellows', 0.5036360025405884),\n",
       " (u'purple', 0.4984946548938751),\n",
       " (u'Yellow', 0.4921160638332367),\n",
       " (u'maroon', 0.47217172384262085)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.most_similar(positive=['yellow', 'positive'], negative=['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'red', 0.751919150352478),\n",
       " (u'bright_yellow', 0.6869138479232788),\n",
       " (u'orange', 0.6421886682510376),\n",
       " (u'blue', 0.6376121044158936),\n",
       " (u'purple', 0.6272757053375244),\n",
       " (u'yellows', 0.612633228302002),\n",
       " (u'pink', 0.6098285913467407),\n",
       " (u'bright_orange', 0.5974606871604919),\n",
       " (u'Warplanes_streaked_overhead', 0.583052396774292),\n",
       " (u'participant_LOGIN', 0.5816755294799805)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.most_similar(positive=['yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word distance from 'positive' and 'negative'\n",
    "\n",
    "I also looked at something simpler - compare the similiarity of a word to a known strongly polar word-pair. Higher similarity to one of them, might indocate the direction of word's polarity. E.g.:\n",
    "sim('happy', 'positive') > sim('happy', 'negative')  \n",
    "hence 'happy' is a positive word.\n",
    "This, of course, does not always give a correct result, but could potentially be good enough for a labelling function. \n",
    "As above, we would need to know first wether a word is polar or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35381865425934766"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('happy', 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15686738467201589"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('happy', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26622150915217901"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('proud', 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076199017451654236"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('proud', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1383159178191852"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('ashamed', 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15910607381262118"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('ashamed', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36027422016349692"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('sad', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42367145004619988"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('sad', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.134985924898754"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('bitter', 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18035062511180683"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('bitter', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12414770358757789"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('green', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17292317890581094"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.similarity('green', 'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using WordNet antonyms\n",
    "\n",
    "Wether a word has an antonym or not might indicate that it has polarity (aka is either positive or negative). For example words such as 'scientific' or 'green' will not have any antonyms, while antonym pairs wil often reflect positive-negative relation, e.g. 'happy-sad' 'proud-'ashamed'\n",
    "The obvious limitations are:\n",
    "1. not every word that has an antonymous word pair represents positive-negative realtion. (e.g. dry-wet; tall-short)\n",
    "2. some more obscure words will not have an antonym even if they are positive/negative (e.g. forlorn)\n",
    "3. This only distinguises between neutral and non-neutral words and not neutral-positive-negative.  \n",
    "\n",
    "Given (3), I think this a better suited idea for a labelling funtion in a polar vs non-polar classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('scientific.a.01')\n",
      "No antonyms\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('scientific.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(),  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('green.s.01')\n",
      "No antonyms\n",
      "No antonyms\n",
      "No antonyms\n",
      "No antonyms\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('green.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(),  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('happy.a.01')\n",
      "(u'happy', ' - ', u'unhappy')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('happy.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sad.a.01')\n",
      "(u'sad', ' - ', u'glad')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('sad.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('elated.a.01')\n",
      "(u'elated', u'dejected')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('elated.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(),  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('proud.a.01')\n",
      "(u'proud', ' - ', u'humble')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('proud.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('ashamed.a.01')\n",
      "(u'ashamed', ' - ', u'unashamed')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('ashamed.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('tall.a.01')\n",
      "(u'tall', ' - ', u'short')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('tall.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('forlorn.s.01')\n",
      "No antonyms\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('forlorn.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('other.a.01')\n",
      "(u'other', ' - ', u'same')\n"
     ]
    }
   ],
   "source": [
    "word = wn.synset('other.a.01')\n",
    "print (word)\n",
    "for l in word.lemmas():\n",
    "    if l.antonyms():\n",
    "        print(l.name(), \" - \",  l.antonyms()[0].name())\n",
    "    else:\n",
    "        print(\"No antonyms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
